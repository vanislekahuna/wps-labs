{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/E7OM0v/VJtj7K4UCfrza",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanislekahuna/wps-labs/blob/main/data/historical_bc_wildfires/Earth_Engine_Augment_Wildfire_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **How to Augment Wildfire Datasets with Historical Weather Data using Python and Google Earth Engine**"
      ],
      "metadata": {
        "id": "NeLBpWAVGfSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://images.unsplash.com/photo-1625284540218-6f8eeeb43778?q=80&w=1170&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" width=600 height=400>\n",
        "\n",
        "[Source](https://unsplash.com/photos/black-and-white-clouds-over-mountain-qmk3g1MpqiA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)"
      ],
      "metadata": {
        "id": "WiRzGdbgxGwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "Picture this: You're a data scientist working with wildfire data, and all you have are basic fire records such as location coordinates, timestamps, and maybe a unique fire ID. While this information tells you where and when a fire occurred, it doesn't tell you why it started or spread the way it did.\n",
        "\n",
        "Weather conditions play a crucial role in wildfire behavior. Temperature, wind speed, humidity, and soil moisture can mean the difference between a small grass fire and a devastating blaze. But manually collecting historical weather data for hundreds or thousands of fire locations? That sounds like a nightmare.\n",
        "\n",
        "This is where the power of Python and Google Earth Engine comes to the rescue! With additional context built in to our code, we're capable of building more sophisticated fire risk model aided by machine learning to incorporate historical weather patterns alongside location and vegetation data, potentially identifying high-risk areas before fire season even begins.\n",
        "\n",
        "The results of this work is crucial as it extends the confines of the labs and offices thinking about these very challenges. For example, of the wide range of sectors we can impact with this engineered dataset, one that you may not have thought about is the insurance industry. Insurance companies can leverage this enriched dataset to develop more accurate wildfire risk premiums by analyzing how specific weather conditions (like low humidity combined with high winds) correlate with fire severity and property damage, leading to more precise underwriting and fairer pricing models. We can also impact emergency management agencies by helping them develop more effective early warning systems through the identification of weather conditions that historically preceded large fires, thus enabling them to issue targeted alerts and deploy resources proactively to save not only property, but most importantly, lives.\n",
        "\n",
        "In this tutorial, we'll build a tool that automatically enriches wildfire datasets with comprehensive weather information, opening up new possibilities for fire risk assessment, insurance modeling, and research applications. By the end of it, you'll have a Python script that can accomplish the following:\n",
        "\n",
        "- Take location data (coordinates + timestamps)  \n",
        "- Automatically fetch historical weather data from Google Earth Engine's ERA5 dataset\n",
        "- Process large datasets efficiently using batch processing\n",
        "- Handle multiple file formats (CSV, Excel, JSON, SQLite)\n",
        "- Export enriched datasets ready for analysis\n",
        "\n",
        "The enriched data will include temperature, wind speed/direction, humidity levels, and soil temperatureâ€Š-â€Šall the environmental context you need for meaningful wildfire analysis. You ready to make big impact with data? Right then. Let's begin."
      ],
      "metadata": {
        "id": "6V73U--fGfPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before diving in, make sure you have:\n",
        "\n",
        "- Python 3.8+ installed\n",
        "- A Google Cloud Project with Earth Engine API enabled\n",
        "- Basic familiarity with pandas and data manipulation\n",
        "\n",
        "### Installing Dependencies\n",
        "\n",
        "First, let's install the required packages by saving them and their desired versions in a file we'll title as `requirements.txt`:\n",
        "\n",
        "```\n",
        "earthengine-api==0.1.406\n",
        "geemap==0.32.1 #Backup version: 0.20.4\n",
        "pandas>=1.3.0\n",
        "python-dotenv>=0.19.0\n",
        "requests>=2.25.0\n",
        "```"
      ],
      "metadata": {
        "id": "-PbmDCEfGfHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then run the following command in a terminal that can run python scripts such as Bash or Anaconda Prompt:\n",
        "\n",
        "```\n",
        "!pip install -r requirements.txt\n",
        "```"
      ],
      "metadata": {
        "id": "FA0P1y_pJSSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "### requirements.txt ###\n",
        "########################\n",
        "\n",
        "%%capture\n",
        "!pip install earthengine-api==0.1.406\n",
        "!pip install geemap==0.32.1 #Backup version: 0.20.4\n",
        "!pip install pandas>=1.3.0\n",
        "!pip install python-dotenv>=0.19.0\n",
        "!pip install requests>=2.25.0"
      ],
      "metadata": {
        "id": "qxOAKhxmHoYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Configuration\n",
        "\n",
        "And if you don't already have a `.env` file in your repository,  create one using the following commands:\n",
        "\n",
        "```bash\n",
        "touch .env\n",
        "start .env\n",
        "```"
      ],
      "metadata": {
        "id": "O97ibUgMIvIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you needed help creating a Google Earth Engine project, lucky for you I've published a [**tutorial**](https://medium.com/towards-artificial-intelligence/how-to-set-up-a-google-earth-engine-cloud-project-fe5472ddbaeb) on exactly how to do so. Be sure to check it out if needed!\n",
        "\n",
        "This approach keeps sensitive information out of your codeâ€”perfect for sharing or open-source projects. Once you have a Google Earth Engine project name in place, edit the `.env` file and add your Google Earth Engine project name in it:\n",
        "\n",
        "```bash\n",
        "PROJECT_NAME=\"insert-your-project-name\"\n",
        "```"
      ],
      "metadata": {
        "id": "VNkfNSHgKhGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the secret project name from userdata\n",
        "from google.colab import userdata\n",
        "\n",
        "project_name = userdata.get('project_name')"
      ],
      "metadata": {
        "id": "tdh9Gg4HKQ1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last but certainly not least, we need to add a `.gitignore` file to our repository using the following commands we saw earlier in the event that we don't already have one:\n",
        "\n",
        "```bash\n",
        "touch .gitignore\n",
        "start .gitignore\n",
        "```\n",
        "\n",
        "The `.gitignore` file acts as a security guard for your repository, preventing Git from tracking, uploading, and displaying sensitive information when you commit code to version control platforms like GitHub. Adding files like `.env` to `.gitignore` is crucial because these files contain sensitive information like your Google Cloud project name, API keys, passwords, and other configuration secrets that should never be publicly visible. Without proper `.gitignore` protection, you could accidentally expose your project credentials to anyone who views your repository, potentially leading to unauthorized access to your Google Earth Engine resources or unexpected billing charges.\n",
        "\n",
        "The `.gitignore` file ensures that only your code gets shared publicly while keeping your personal configuration and credentials safely on your local machine. If you're curious as to what a `.gitignore` file looks like or how it should be structured, feel free to [check out the template](https://github.com/bcgov/wps-tutorials/blob/main/.gitignore) I've used for this project."
      ],
      "metadata": {
        "id": "f9cxaBA1cMqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Data Requirements\n",
        "\n",
        "The last prerequisite we'll need when feeding data to our script is that it expects the wildfire data to contain some version of these four essential columns:\n",
        "\n",
        "| Column | Description | Example |\n",
        "|--------|-------------|---------|\n",
        "| `LATITUDE` | Fire location latitude | `34.0522` |\n",
        "| `LONGITUDE` | Fire location longitude | `-118.2437` |\n",
        "| `FIRE_ID` | Unique fire identifier | `FIRE_2023_CA_001` |\n",
        "| `DATE_COLUMN` | Date/time information | `20230715` or `20230715143000` |\n",
        "\n",
        "Note that the date format is flexible as it can handle both `YYYYMMDD` (date only) and `YYYYMMDDHHMMSS` (date and time) formats. For our script to run, we'll need either the file path or the link pointing to the dataset you plan on processing."
      ],
      "metadata": {
        "id": "M84RG93iGfE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the core components\n",
        "\n",
        "Now that we've laid the groundwork to successfully (and safely) run this tutorial, let's go through out Python code piece by piece so we understand how we're extracting this historical weather data:\n",
        "\n",
        "### 1. Date Conversion Utility\n",
        "\n",
        "One challenge with wildfire datasets is inconsistent date formats. With the code below, let's create a robust date converter function which can gracefully handles different date formats and missing values essential for real-world data processing:"
      ],
      "metadata": {
        "id": "iJh0DrU5GfCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import geemap\n",
        "import logging\n",
        "import math\n",
        "import random\n",
        "import requests\n",
        "import os\n",
        "import sqlite3\n",
        "import time\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "qcDdITwsO73F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_float_to_datetime(series):\n",
        "    \"\"\"\n",
        "    Converts a pandas Series of float-based date values to datetime.\n",
        "    Handles:\n",
        "    - 8-digit formats (YYYYMMDD)\n",
        "    - 14-digit formats (YYYYMMDDHHMMSS)\n",
        "    - Invalid lengths become NaT\n",
        "    \"\"\"\n",
        "    # Initialize output series with NaT (same index/dtype as input)\n",
        "    datetime_series = pd.Series(index=series.index,\n",
        "                               data=series, # data=pd.NaT,\n",
        "                               dtype='datetime64[ns]')\n",
        "\n",
        "    # Process non-null values\n",
        "    non_null = series.dropna()\n",
        "    if non_null.empty:\n",
        "        return datetime_series\n",
        "\n",
        "    try:\n",
        "        # Convert to integer then string (avoids scientific notation)\n",
        "        str_dates = non_null.astype('int64').astype(str)\n",
        "\n",
        "        # Identify valid date lengths\n",
        "        mask_8 = str_dates.str.len() == 8    # Date only\n",
        "        mask_14 = str_dates.str.len() == 14  # Date + time\n",
        "\n",
        "        # Parse valid formats\n",
        "        parsed_dates = pd.concat([\n",
        "            pd.to_datetime(str_dates[mask_8], format='%Y%m%d', errors='coerce'),\n",
        "            pd.to_datetime(str_dates[mask_14], format='%Y%m%d%H%M%S', errors='coerce')\n",
        "        ]).sort_index()  # Maintain original order\n",
        "\n",
        "        # Update result series with valid dates\n",
        "        datetime_series.update(parsed_dates)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Conversion error: {str(e)}\")\n",
        "\n",
        "    return datetime_series"
      ],
      "metadata": {
        "id": "MKVgVPzhGeyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've also created a `create_dataframe()` function serves as a universal data loader that intelligently handles multiple file formats (CSV, Excel, JSON, SQLite) from both local file paths and remote URLs, making it extremely user-friendly for loading wildfire datasets regardless of how they're stored. It automatically detects the file type based on the file extension and applies the appropriate pandas reading method, while also providing interactive prompts to guide users through the data loading process."
      ],
      "metadata": {
        "id": "3WzVWKmtihbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataframe(hardcoded_path=None, sheet_name=None, db_name=None, table_name=None):\n",
        "    \"\"\"\n",
        "    Create a Pandas DataFrame from a file path or a link.\n",
        "\n",
        "    Parameters:\n",
        "    - hardcoded_path (str, optional): The file path or link to be used. If None, the user will be prompted to enter it.\n",
        "    - sheet_name (str, optional): The sheet name for Excel files or table name for SQL databases.\n",
        "    - db_name (str, optional): The database name for SQL databases.\n",
        "    - table_name (str, optional): The table name for SQL databases.\n",
        "    \"\"\"\n",
        "    if hardcoded_path is None:\n",
        "        filepath_or_link = input(\"/n Please enter the file path or link for the data source: \")\n",
        "    else:\n",
        "        filepath_or_link = hardcoded_path\n",
        "\n",
        "    try:\n",
        "        # Handle web URL\n",
        "        if filepath_or_link.startswith(('http://', 'https://')):\n",
        "            if filepath_or_link.endswith('.csv'):\n",
        "                df = pd.read_csv(filepath_or_link)\n",
        "\n",
        "            elif filepath_or_link.endswith(('.xls', '.xlsx')):\n",
        "                df = pd.read_excel(filepath_or_link, sheet_name=sheet_name)\n",
        "\n",
        "            elif filepath_or_link.endswith('.json'):\n",
        "                df = pd.read_json(filepath_or_link)\n",
        "\n",
        "            elif filepath_or_link.endswith('.db'):\n",
        "                response = requests.get(filepath_or_link)\n",
        "                db_name = input(\"\\n Please enter the DATABASE NAME ONLY (do not include the .db extension): \")\n",
        "                table_name = input(f\"\\n Please enter the specific TABLE NAME you wish to access in the {db_name} database: \")\n",
        "\n",
        "                with open(db_name, \"wb\") as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "                conn = sqlite3.connect(db_name)\n",
        "                query = f\"SELECT * FROM {table_name}\" if table_name else \"SELECT * FROM sqlite_master WHERE type='table';\"\n",
        "                df = pd.read_sql_query(query, conn)\n",
        "                conn.close()\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format from the URL.\")\n",
        "\n",
        "        else:\n",
        "            # Handle local file path\n",
        "            filepath_or_link = os.path.expanduser(filepath_or_link)\n",
        "            if not os.path.exists(filepath_or_link):\n",
        "                raise FileNotFoundError(f\"The file {filepath_or_link} does not exist.\")\n",
        "\n",
        "            if filepath_or_link.endswith('.csv'):\n",
        "                df = pd.read_csv(filepath_or_link)\n",
        "\n",
        "            elif filepath_or_link.endswith(('.xls', '.xlsx')):\n",
        "                df = pd.read_excel(filepath_or_link, sheet_name=sheet_name)\n",
        "\n",
        "            elif filepath_or_link.endswith('.json'):\n",
        "                df = pd.read_json(filepath_or_link)\n",
        "\n",
        "            elif filepath_or_link.endswith('.db'):\n",
        "                conn = sqlite3.connect(filepath_or_link)\n",
        "                query = f\"SELECT * FROM {table_name}\" if table_name else \"SELECT * FROM sqlite_master WHERE type='table';\"\n",
        "                df = pd.read_sql_query(query, conn)\n",
        "                conn.close()\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format.\")\n",
        "\n",
        "        print(\"\\n âœ… DataFrame created successfully:\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n ðŸ™„ An error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "B9pYSjLxhHzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Helper Functions\n",
        "\n",
        "With our utilitarian functions out of the way, let's talk about accessing Google's Earth Engine API! The heart of our tool is the `get_weather_data()` function we've written to query Google's Earth Engine for weather data. The aim of this function is to handle the complex process of querying satellite data, converting units, and calculating derived metrics like wind speed from component vectors.\n",
        "\n",
        "And within `get_weather_data()` are **helper functions** like the `sample_point_data()` function that handles the technical complexity of actually extracting weather data from Google Earth Engine at specific coordinates, including error handling when satellite data isn't available at a location. It abstracts away the Earth Engine API intricacies, allowing `get_weather_data()` to focus on data processing logic rather than API management details."
      ],
      "metadata": {
        "id": "kSkSWbm0P6-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_point_data(weather_data, point, lat, lon, date_val):\n",
        "    \"\"\"\n",
        "    Sample Earth Engine data at a specific point.\n",
        "\n",
        "    Args:\n",
        "        weather_data: Earth Engine Image with weather variables\n",
        "        point: Earth Engine Geometry point\n",
        "        lat: Latitude value for logging\n",
        "        lon: Longitude value for logging\n",
        "        date_val: Date value for logging\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of sampled data or None if no data available\n",
        "    \"\"\"\n",
        "    sample_result = weather_data.sample(point, 30).first()\n",
        "\n",
        "    # Check if sample_result is null\n",
        "    if sample_result is None or ee.Algorithms.IsEqual(sample_result, None).getInfo():\n",
        "        logger.warning(f\"No data at point ({lat}, {lon}) for date {date_val}\")\n",
        "        return None\n",
        "\n",
        "    # Convert to dictionary\n",
        "    return sample_result.toDictionary().getInfo()"
      ],
      "metadata": {
        "id": "_tdTrBVoRLg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my opinion, the magic with this process is our `retry_with_backoff()` decorator function engineered to supports the `sample_point_data()` function by automatically retrying failed API calls to Google Earth Engine. However, what's interesting about this function is that it increases the delay between attempts, making the weather data extraction process more robust against temporary network issues or API timeouts. It also implements exponential backoff (doubling the wait time after each failure) and adds random jitter to prevent multiple processes from overwhelming the API simultaneously when retrying. This function is crucial for handling the inherent instability of remote API calls, ensuring that temporary hiccups don't derail the entire weather data enrichment process when processing large wildfire datasets."
      ],
      "metadata": {
        "id": "tR_qiRiHkIVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retry_with_backoff(max_retries=3, initial_delay=2, backoff_factor=2, exceptions=(ee.EEException,)):\n",
        "    \"\"\"\n",
        "    Decorator to retry a function with exponential backoff.\n",
        "\n",
        "    Args:\n",
        "        max_retries: Maximum number of retries before giving up\n",
        "        initial_delay: Initial delay between retries in seconds\n",
        "        backoff_factor: Factor to multiply delay by after each retry\n",
        "        exceptions: Tuple of exceptions to catch and retry on\n",
        "\n",
        "    Returns:\n",
        "        A decorator function\n",
        "    \"\"\"\n",
        "    def decorator(func):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            retries = 0\n",
        "            delay = initial_delay\n",
        "\n",
        "            while retries <= max_retries:\n",
        "                try:\n",
        "                    return func(*args, **kwargs)\n",
        "                except exceptions as e:\n",
        "                    if retries == max_retries:\n",
        "                        # If we've hit max retries, re-raise the exception\n",
        "                        raise\n",
        "\n",
        "                    # Check if it's a timeout error\n",
        "                    if \"timed out\" in str(e).lower():\n",
        "                        # Add jitter to prevent synchronized retries\n",
        "                        jitter = random.uniform(0, 0.1 * delay)\n",
        "                        sleep_time = delay + jitter\n",
        "\n",
        "                        logger.warning(f\"Timeout error, retrying in {sleep_time:.1f} seconds... (Attempt {retries+1}/{max_retries})\")\n",
        "                        time.sleep(sleep_time)\n",
        "\n",
        "                        # Increase delay for next retry\n",
        "                        delay *= backoff_factor\n",
        "                        retries += 1\n",
        "                    else:\n",
        "                        # Not a timeout error, re-raise immediately\n",
        "                        raise\n",
        "        return wrapper\n",
        "    return decorator"
      ],
      "metadata": {
        "id": "3W8BWY2KRLuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the retry decorator to the sampling function\n",
        "sample_point_data_with_retry = retry_with_backoff()(sample_point_data)"
      ],
      "metadata": {
        "id": "wsSEK5veS6bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another important helper function is `wind_direction_to_text()` which transforms the calculated wind direction from degrees (like 245.8Â°) into human-readable cardinal directions (like \"Southwest\"), making the output dataset more interpretable for analysts and researchers."
      ],
      "metadata": {
        "id": "T-EwktoliaYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wind_direction_to_text(wind_dir_deg):\n",
        "    \"\"\"\n",
        "    Convert wind direction in degrees to 8-point cardinal direction.\n",
        "\n",
        "    Args:\n",
        "        wind_dir_deg (float): Wind direction in degrees (0-360)\n",
        "\n",
        "    Returns:\n",
        "        str: Cardinal direction as text (N, NE, E, SE, S, SW, W, NW)\n",
        "    \"\"\"\n",
        "    # Define direction ranges and corresponding text\n",
        "    directions = [\n",
        "        (337.5, 360, \"North\"),\n",
        "        (0, 22.5, \"North\"),\n",
        "        (22.5, 67.5, \"Northeast\"),\n",
        "        (67.5, 112.5, \"East\"),\n",
        "        (112.5, 157.5, \"Southeast\"),\n",
        "        (157.5, 202.5, \"South\"),\n",
        "        (202.5, 247.5, \"Southwest\"),\n",
        "        (247.5, 292.5, \"West\"),\n",
        "        (292.5, 337.5, \"Northwest\")\n",
        "    ]\n",
        "\n",
        "    # Normalize the degree to be between 0 and 360\n",
        "    wind_dir_deg = wind_dir_deg % 360\n",
        "\n",
        "    # Find the matching direction\n",
        "    for start, end, direction in directions:\n",
        "        if (start <= wind_dir_deg < end) or (start <= wind_dir_deg <= end and end == 360):\n",
        "            return direction\n",
        "\n",
        "    # This should never happen if the ranges are correct\n",
        "    return \"Unknown\""
      ],
      "metadata": {
        "id": "tCxw6CF-Sw_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Together, these functions separate concerns where the former handles the data extraction complexity while the latter enhances data usabilityâ€”allowing `get_weather_data()` to orchestrate the overall weather enrichment process cleanly:"
      ],
      "metadata": {
        "id": "LzdQZqRflNq0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZGW9aZ0_SP2"
      },
      "outputs": [],
      "source": [
        "def get_weather_data(row, id_col):\n",
        "    \"\"\"\n",
        "    Extract weather data from Google Earth Engine for a specific location and time.\n",
        "\n",
        "    Args:\n",
        "        row: DataFrame row containing 'ignition_datetime', 'LATITUDE', and 'LONGITUDE'\n",
        "\n",
        "    Returns:\n",
        "        dict: Weather data or NaN values if data cannot be retrieved\n",
        "    \"\"\"\n",
        "    # Validate input data\n",
        "    date_val = row.get('ignition_datetime')\n",
        "    lat = row.get('LATITUDE')\n",
        "    lon = row.get('LONGITUDE')\n",
        "    fire_label = row.get(id_col)\n",
        "\n",
        "    # Initialize default return values\n",
        "    default_values = {\n",
        "        'temperature_c': np.nan,\n",
        "        'wind_speed_ms': np.nan,\n",
        "        'wind_direction_deg': np.nan,\n",
        "        'wind_direction': 'No data returned',\n",
        "        'humidity_dewpoint_temperature_2m': np.nan,\n",
        "        'soil_temperature_level_1': np.nan,\n",
        "        'fire_label': fire_label,\n",
        "        'ignition_datetime': date_val\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Check if we have all required values\n",
        "        if date_val is None or pd.isna(date_val) or not isinstance(date_val, datetime):\n",
        "            logger.warning(f\"Fire label {fire_label} has an invalid ignition_datetime: {date_val}\")\n",
        "            return default_values\n",
        "\n",
        "        if lat is None or pd.isna(lat) or lon is None or pd.isna(lon):\n",
        "            logger.warning(f\"Fire label {fire_label} has an invalid coordinates: lat={lat}, lon={lon}\")\n",
        "            return default_values\n",
        "\n",
        "        # Convert datetime to Earth Engine format\n",
        "        date = ee.Date(date_val)\n",
        "\n",
        "        # Create point geometry\n",
        "        point = ee.Geometry.Point([lon, lat])\n",
        "\n",
        "        # Get ERA5 reanalysis data\n",
        "        # era5 = ee.ImageCollection('ECMWF/ERA5/HOURLY')\n",
        "        era5 = ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY')\n",
        "\n",
        "        # Filter to the date (add buffer to ensure we get data)\n",
        "        start_date = date.advance(-1, 'hour')\n",
        "        end_date = date.advance(2, 'hour')\n",
        "        era5_filtered = era5.filterDate(start_date, end_date)\n",
        "\n",
        "        # Check if we have any images\n",
        "        if era5_filtered.size().getInfo() == 0:\n",
        "            logger.warning(f\"No ERA5 data found for time range around {date_val} for the {fire_label} fire label\")\n",
        "            # return None\n",
        "            return default_values\n",
        "\n",
        "        # Get the image closest to our target time\n",
        "        era5_list = era5_filtered.toList(era5_filtered.size())\n",
        "        era5_img = ee.Image(era5_list.get(0))  # Get first image\n",
        "\n",
        "        # Extract weather variables at the point (using resample for faster computation)\n",
        "        weather_data = era5_img.select(\n",
        "            ['temperature_2m', 'u_component_of_wind_10m',\n",
        "             'v_component_of_wind_10m', 'dewpoint_temperature_2m', 'soil_temperature_level_1']).resample(\"bilinear\")\n",
        "\n",
        "        # Sample the point with error handling and retry\n",
        "        try:\n",
        "            data = sample_point_data_with_retry(weather_data, point, lat, lon, date_val)\n",
        "\n",
        "            # Check if data is empty\n",
        "            if not data:\n",
        "                logger.warning(f\"Empty data returned for ({lat}, {lon}) at {date_val} for the {fire_label} fire label\")\n",
        "                return default_values\n",
        "\n",
        "            # Calculate wind speed and direction from u,v components\n",
        "            u = data.get('u_component_of_wind_10m', 0)\n",
        "            v = data.get('v_component_of_wind_10m', 0)\n",
        "            wind_speed = (u**2 + v**2)**0.5\n",
        "\n",
        "            # Avoid division by zero or undefined math\n",
        "            if u == 0 and v == 0:\n",
        "                wind_dir = 0  # No wind\n",
        "            else:\n",
        "                wind_dir = (270 - (180/3.14159) * math.atan2(v, u)) % 360\n",
        "\n",
        "            # Convert temperature from K to C (handle None values)\n",
        "            temp_k = data.get('temperature_2m')\n",
        "            temp_c = temp_k - 273.15 if temp_k is not None else np.nan\n",
        "\n",
        "            return {\n",
        "                'temperature_c': temp_c,\n",
        "                'wind_speed_ms': wind_speed,\n",
        "                'wind_direction_deg': wind_dir,\n",
        "                'wind_direction': wind_direction_to_text(wind_dir),\n",
        "                'humidity_dewpoint_temperature_2m': data.get('dewpoint_temperature_2m'),\n",
        "                'soil_temperature_level_1': data.get('soil_temperature_level_1'),\n",
        "                'fire_label': fire_label,\n",
        "                'ignition_datetime': date_val\n",
        "            }\n",
        "\n",
        "        except ee.EEException as e:\n",
        "            logger.error(f\"Earth Engine sampling error for ({lat}, {lon}) at {date_val}: {str(e)}\")\n",
        "            return default_values\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing row: {str(e)}\")\n",
        "        # For debugging in development\n",
        "        # import traceback\n",
        "        # logger.error(traceback.format_exc())\n",
        "        return default_values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Extraction\n",
        "\n",
        "Now that we've understood how the data is extracted from the Google Earth Engine, let's now have a look at the function orchestrating this entire process. The `process_dataframe()` function is the workhorse that breaks large wildfire datasets into manageable batches (default 100 records) to avoid overwhelming Google Earth Engine's API limits while providing real-time progress updates. It includes smart error handling that lets processing continue even if individual batches fail, and automatically saves each batch's results to prevent data loss if the process gets interrupted. The function also incorporates mandatory delays between batches to respect API rate limits, ensuring smooth and reliable weather data extraction."
      ],
      "metadata": {
        "id": "lzGANzOZKwdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataframe(df, batch_size=100, batch_delay=3, id_col=None):\n",
        "    \"\"\"\n",
        "    Process the dataframe in batches to avoid Earth Engine quota issues.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with wildfire data\n",
        "        batch_size: Number of rows to process in each batch\n",
        "        batch_delay: Delay in seconds between processing batches\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with added weather data\n",
        "    \"\"\"\n",
        "\n",
        "    df = df[df[\"ignition_datetime\"].notna()].sort_values(\"ignition_datetime\")\n",
        "\n",
        "    results = []\n",
        "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
        "\n",
        "    # Process in batches\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        batch = df.iloc[i:i+batch_size]\n",
        "        batch_num = i//batch_size + 1\n",
        "\n",
        "        # Clear progress reporting\n",
        "        print(f\"\\n Processing batch {batch_num} of {total_batches} (rows {i} to {min(i+batch_size-1, len(df)-1)})\")\n",
        "        logger.info(f\"\\n Processing batch {batch_num} of {total_batches} (rows {i} to {min(i+batch_size-1, len(df)-1)})\")\n",
        "\n",
        "        # Apply to each row in this batch\n",
        "        batch_results = batch.apply(get_weather_data, axis=1, result_type='expand', args=(id_col,))\n",
        "\n",
        "        ### ADDITIONAL CHECK ###\n",
        "        # Check if 'temp_c' key exists in the dictionary and is not empty # or not(bool(batch_results['temperature_c']))\n",
        "        if 'temperature_c' not in batch_results or (len(batch_results['temperature_c'].value_counts()) == 0):\n",
        "            print(f\"Skipping batch {batch_num} of {total_batches} - no temperature data available \\n\\n\")\n",
        "            continue  # Skip to the next iteration of the loop\n",
        "\n",
        "        else:\n",
        "          results.append(batch_results)\n",
        "\n",
        "          # Add progress information\n",
        "          print(f\"\\n Completed batch {batch_num}/{total_batches} ({batch_num/total_batches*100:.1f}%) \\n\")\n",
        "          logger.info(f\"Completed batch {batch_num}/{total_batches} ({batch_num/total_batches*100:.1f}%)\")\n",
        "\n",
        "          # Saving each batch to ensure we don't waste computation:\n",
        "          download_name = f\"weather_data_batch_{batch_num}_of_{total_batches}.csv\"\n",
        "          save_results_to_downloads(batch_results, filename=download_name)\n",
        "\n",
        "        # Add a delay between batches to reduce pressure on the API\n",
        "        if batch_num < total_batches:\n",
        "            print(f\"\\n Pausing for {batch_delay} second(s) before next batch... \\n\\n\")\n",
        "            logger.info(f\"Pausing for {batch_delay} second(s) before next batch...\")\n",
        "            time.sleep(batch_delay)\n",
        "\n",
        "    # Combine all batches\n",
        "    if results:\n",
        "        print(\"\\n Concatenating weather results... \\n\\n\")\n",
        "        logger.info(\"Concatenating weather results...\")\n",
        "        weather_data = pd.concat(results)\n",
        "\n",
        "        # Force completion of all pending operations\n",
        "        print(\"\\n Finalizing all Earth Engine operations...\")\n",
        "        ee.data.computeValue(ee.Number(1))  # This forces a sync point\n",
        "\n",
        "        print(\"\\n Weather data processing complete.\")\n",
        "        logger.info(\"Weather data processing complete.\")\n",
        "        return weather_data\n",
        "\n",
        "    else:\n",
        "        print(\"No weather data to process.\")\n",
        "        logger.warning(\"No weather data to process.\")\n",
        "        return df"
      ],
      "metadata": {
        "id": "_yIodKgeKKaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And for each batch that's processed, we'll rely on the `save_results_to_downloads()` function to download our result in the scenario that an error occurs as we're scraping data. The function was engineered to handles the file management aspect by dynamically creating a \"temp_downloads\" folder in the user's Downloads directory (or any specified location) so that we get immediate feedback from the output files and continuously save our progress in an easily accessible location."
      ],
      "metadata": {
        "id": "NANaJwofmq2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results_to_downloads(weather_data, filename='weather_data.csv', save_folder='temp_downloads'):\n",
        "    \"\"\"\n",
        "    Save results directly to temporary downloads folder\n",
        "\n",
        "    Args:\n",
        "        weather_data: DataFrame to save\n",
        "        filename: Name of the file to save\n",
        "\n",
        "    Returns:\n",
        "        Path where the file was saved\n",
        "    \"\"\"\n",
        "\n",
        "    # Save to new folder in Downloads or create it if it doesn't exist\n",
        "    relative_path = '~/Downloads'\n",
        "    expand = os.path.expanduser(relative_path)\n",
        "    save_path = f'{expand}/{save_folder}'\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "\n",
        "    # Create full file path\n",
        "    full_path = os.path.join(save_path, filename)\n",
        "\n",
        "    # Save the DataFrame\n",
        "    weather_data.to_csv(full_path, index=False)\n",
        "\n",
        "    print(f\"âœ… Data successfully saved to: {full_path}\")"
      ],
      "metadata": {
        "id": "GuKHpnHVmqqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, our batch processing approach provides several benefits:\n",
        "- **Progress tracking**: Users can see processing status\n",
        "- **Error resilience**: Failed batches don't stop the entire process\n",
        "- **Rate limiting**: Prevents overwhelming the API\n",
        "- **Resume capability**: Individual batch saves allow restarting from failures"
      ],
      "metadata": {
        "id": "znU37c9zTCL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Script Execution\n",
        "Now that we've discussed how all the individual functions play their role in the data extraction process, we can now enact the final step of bringing everything we learned together in the main execution script:"
      ],
      "metadata": {
        "id": "gkurxSSDTJQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # # Load environment variables\n",
        "    # load_dotenv()\n",
        "\n",
        "    # # Authentication\n",
        "    # # Get the secret project name from userdata\n",
        "    # project_name = os.environ['PROJECT_NAME']\n",
        "    # if not project_name:\n",
        "    #     raise ValueError(\"PROJECT_NAME environment variable is required. Please set it in your .env file.\")\n",
        "\n",
        "    # Trigger the authentication flow.\n",
        "    ee.Authenticate()\n",
        "\n",
        "    # Initialize the library.\n",
        "    ee.Initialize(project=project_name)\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = create_dataframe() #[:200]\n",
        "\n",
        "    datetime_col = input(\"\\n Enter the column name of the DATETIME column in your dataset: \")\n",
        "\n",
        "    df[\"ignition_datetime\"] = convert_float_to_datetime(df[datetime_col])\n",
        "\n",
        "    unique_id_col = input(\"\\n Enter the column name of the column you wish to designate as the UNIQUE ID field for your dataset: \")\n",
        "\n",
        "    weather_data = process_dataframe(df, id_col=unique_id_col)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Execution took {end_time - start_time:.2f} seconds, or {((end_time - start_time) / 60):.2f} minutes, or {((end_time - start_time) / 3600):.2f} hours\")\n",
        "\n",
        "    save_results_to_downloads(weather_data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "MRq6YUycTKbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "Voila! You now have a powerful tool for enriching wildfire datasets with weather information. This opens up numerous possibilities for analysis and modeling. Consider extending the tool to include:\n",
        "\n",
        "- Additional weather variables (precipitation, atmospheric pressure)\n",
        "- Vegetation indices from satellite imagery\n",
        "- Topographical data (elevation, slope)\n",
        "- Historical fire perimeter data\n",
        "\n",
        "The combination of location, time, weather, and environmental data creates rich datasets perfect for machine learning applications in wildfire research and risk management."
      ],
      "metadata": {
        "id": "IoHrwMowTCDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tips for Success\n",
        "\n",
        "- **Start Small**: Begin with a subset of your data (maybe 50-100 records) to test the process and understand processing times.\n",
        "\n",
        "- **Monitor Your Quotas**: Google Earth Engine has usage limits. The batch processing and delays help, but keep an eye on your consumption.\n",
        "\n",
        "- **Clean Your Data First**: Validate coordinates and dates before processing. Invalid data will slow you down and waste API calls.\n",
        "\n",
        "- **Save Everything**: The script automatically saves batch results, but consider backing up your original data too."
      ],
      "metadata": {
        "id": "7DhLAzKSUGqK"
      }
    }
  ]
}